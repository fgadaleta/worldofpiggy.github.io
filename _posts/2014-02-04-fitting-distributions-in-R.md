---
layout: post
title: Fitting distributions in R
comments: true
tags: [code, distribution, fitting, R, statistics]
---

One recurrent problem that we are experiencing in the office is, as the title
suggests, about fitting statistical distributions. Given data from physical
experiments, or somehow related to them (such as be gene expression profiles
or parameters of a gene selection method), we are spending some time to figure
out the distribution that generated those data. Well, not just a distribution.
Just the best one. One naive way of doing this is, of course, to plot the data
(e.g. plot the histogram) and inspect the distribution in a graphical way. For
instance the R code below

    
    
    dataN <- rnorm(1000, mean=3, sd= 7.5) 
    hist(dataN, breaks=15)

![dataN](https://s3-eu-west-1.amazonaws.com/wopcontent/uploads/2014/02/datan.jpeg) would
plot a gaussian-like histogram, suggesting that the data have been indeed
generated by a normal distribution.

    
    
    dataP <- rpois(n=1000, lambda=4.9)
    hist(dataP, breaks=15)

![dataP](https://s3-eu-west-1.amazonaws.com/wopcontent/uploads/2014/02/datap.jpeg)would
plot the histogram of a poisson-like distribution, indeed. Yet another example
(trying not to be annoying here) would assume that the shape of the
distribution is a Gamma

    
    
    dataG <- rgamma(1000, shape=1.2, scale=1.3) 
    hist(dataG, breaks=15)

![dataG](https://s3-eu-west-1.amazonaws.com/wopcontent/uploads/2014/02/datag.jpeg)So far
so good. Once we are _"sure"_ about the shape of the distribution, the next
big step is to guess the right parameters that govern the real trends within
that distribution. Therefore, in the case of a Normal distribution we should
estimate the right $latex mu $ and $latex sigma $, or for a Poisson
distribution we might need to estimate the $latex lambda $ parameter. If our
assumptions are all about a Gamma distribution, then parameters $latex k$ and
$latex theta $ should be estimated.

## How about the shape?

Even though it's quite nice to see curves on the screen, the graphical way of
guessing the best distribution of unknown data, is not really the most
suitable one. Quantile-quantile plots are much better in that sense. It's
still a graphical technique. But much more explicit since it plots empirical
quantiles and theoretical quantiles on the two axes, giving a straight line in
case the two match consistently. Therefore:

    
    
    qqplot(dataN, rnorm(1000, 3, 7.5), main="QQ-plot Normal") 
    qqplot(dataP, rpois(1000, 4.9), main="QQ-plot Poisson") 
    qqplot(dataG, rgamma(1000, 1.2, 1.3), main="QQ-plot Gamma")

would plot (almost) a straight line, or points aligned onto it. [caption
id="attachment_1704" align="alignnone" width="640"][![Quantile-quantile plot
of a gamma](https://s3-eu-west-1.amazonaws.com/wopcontent/uploads/2014/02/qqg.jpeg)](https://s3-eu-west-1.amazonaws.com/wopcontent/uploads/2014/02/qqg.jpeg) Quantile-quantile plot of a gamma[/caption]

## How about the parameters?

This is indeed the biggest deal. Statisticians know about several methods to
estimate parameters of a distribution, assuming the shape has been correctly
identified. One is the method of moments in which all parameters sampled
directly from data will be assumed as parameters of the candidate
distribution. For instance, if we assume that data have been generated from a
Normal distribution, we should estimate the mean and the variance. Who better
than the sample mean and sample variance could do that? An approximation would
be: $latex hat{mu} = mu &amp;s=2$ and $latex hat{sigma} = sigma &amp;s=2$ that
will be thrown into the normal distribution analytical form we are familiar
with $latex f(x,mu, sigma) = frac{1}{sqrt{2pi}sigma} e^{-
frac{1(x-mu)^2)}{2sigma^2} }&amp;s=2$ Another method I personally prefer is
the Maximum Likelihood Estimation. Of course it relies on the assumed
distribution, as the method of moments. Here is an analytical example for the
Gamma distribution. The maximum likelihood estimation method consists in
maximising the likelihood function which is (in general for any distribution)
$latex L(x_2, ..., x_n,theta) = prod_{i=1}^{n} f(x_i,theta)&amp;s=2$ The MLE
consists in finding the $latex theta &amp;s=2$ that miximises the likelihood
(or equivalently its logarithmic function). For the Gamma distribution it
would be something like $latex L(x_2, ..., x_n, alpha, lambda) =
prod_{i=1}^{n} f(x_i,theta) = prod_{i=1}^{n}
frac{lambda^alpha}{Gamma(alpha)}x_{i}^{alpha-1}e^{-lambda x_i} =
frac{lambda^alpha}{Gamma(alpha)}(prod_{i=1}^{n} x_i)^{alpha-1} e^{-lambda
sum_{i=1}^n x_i} &amp;s=1$ and the logarithmic function is $latex log(L) = n
alpha log(lambda) -n log(Gamma(alpha)) + (alpha-1) sum log(x_i) - lambda sum
x_i &amp;s=1$ As a Pig in love with Math I would keep it like that. But my
room mate, the engineer, usually dislikes that form and prefers to see it in a
more _"useful"_ way, as he claims, whenever he applies this masterpiece to
real data. In R there is a function that does all that, in just one line of
code

    
    
    library(MASS)
     fd_g <- fitdistr(dataG, "gamma")
     est_shape = fd_g$estimate[[1]]
     est_rate = fd_g$estimate[[2]]

The estimated parameters of the assumed Gamma distribution are stored in
_est_shape_ and _est_rate_. I have the shape, I have the parameters. What now?
The final step of any distribution fitting procedure consists in measuring how
good the estimated distribution (shape and parameters) fit the data. Several
possibilities are offered by mathematical statistics that usually go under the
name of _goodness of fit_. These measures basically match the empirical
frequencies of observed data with the ones fitted by a theoretical
distribution (the assumed one). The literature is offering absolute and
relative measures. Two absolute measures are $latex gf_1 = frac{sum_{i=1}^n
|y_i - y^{*}_i| }{n} &amp;s=2$ $latex gf_2 = sqrt{frac{sum (y_i - y^{*}_i)^2
}{n}} &amp;s=2$ where $latex y_i &amp;s=2$ and $latex y^{*}_i &amp;s=2$ are
the empirical frequencies and the theoretical ones respectively. Some relative
measures are: $latex gf_3 = frac{gf_1} {sum frac{y_i}{n} } &amp;s=2$ $latex
gf_4 = frac{gf_2} {sum frac{y_i}{n} } &amp;s=2$ $latex gf_5 = frac{gf_2}
{sqrt{sum frac{y^{2}_i}{n}}}&amp;s=2 $ Again, I found the engineer somewhere
between disgusted and scared. He wanted me to pull some numbers out and he
translated this in R code

    
    
    #assume dataP come from a Poisson distribution
    emp_freq <- table(dataP)       # table of empirical frequencies
    freq <- vector()               # vector of empirical frequencies
    for(i in 1:length(emp_freq))
         freq[i] <- emp_freq[[i]]
    # vector of expected frequencies
    exp_freq <- dpois(0:max(dataP), lambda=est_lambda) 
    gf_a <- mean(abs(freq - trunc(exp_freq)))  #absolute goodness of fit
    gf_r <- gf_a/mean(freq) * 100   # relative goodness of fit 

Even though hypothesis testing might reveal also the significance of the
difference between the two distributions under investigation, the two measures
explained above are  usually good indicators of how good (or bad) the assumed
distribution is.  Next time we'll try to put everything together and write
some code that automatically selects the best in a list of distributions given
as candidates. (oo)
